# âœ… Railway + Ollama + MythoMax-13B

Deploy uncensored AI model (thebloke/mythomax-l2-13b) via Railway.

---

## ğŸŒ API Endpoint

POST <https://o1win-o1win.up.railway.app/api/generate>

---

## ğŸ“¥ Example Payload

```json
{
  "model": "mythomax-l2-13b",
  "prompt": "Explain God-Tier Agentic AI.",
  "stream": false
}
```

ğŸ§ª Python Test

```python
import requests

url = "https://o1win-o1win.up.railway.app/api/generate"
payload = {
    "model": "mythomax-l2-13b",
    "prompt": "Explain God-Tier Agentic AI.",
    "stream": False
}
resp = requests.post(url, json=payload)
print(resp.json())
```

âš ï¸ Notes
No need to RUN ollama pull in Dockerfile.

First request may take time (model lazy-load).

Once cached, responses are fast.

---

## ğŸ§  à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™ Push à¹ƒà¸«à¹‰à¹€à¸ªà¸£à¹‡à¸ˆ

```bash
git clone https://github.com/tantitplozz/o1win.git
cd o1win

# à¸ªà¸£à¹‰à¸²à¸‡ Dockerfile
echo 'FROM ollama/ollama:latest
CMD ["serve"]' > Dockerfile

# à¸ªà¸£à¹‰à¸²à¸‡ README.md
# <à¸§à¸²à¸‡à¹€à¸™à¸·à¹‰à¸­à¸«à¸² README.md à¸”à¹‰à¸²à¸™à¸šà¸™à¸¥à¸‡à¹„à¸› à¸«à¸£à¸·à¸­à¹ƒà¸Šà¹‰ VSCode à¹à¸›à¸°à¹„à¸”à¹‰à¹€à¸¥à¸¢>

# Push
git add .
git commit -m "God-Tier Init: Ollama + MythoMax-13B"
git push origin main
```

ğŸš€ à¸à¸£à¹‰à¸­à¸¡à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¹à¸¥à¹‰à¸§à¸—à¸µà¹ˆ:

```bash
https://o1win-o1win.up.railway.app/api/generate
```
